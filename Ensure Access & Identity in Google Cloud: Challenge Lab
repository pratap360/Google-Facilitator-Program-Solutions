Where like this <Project ID> Project is written it means you need to replace that with your GCP project ID


1st TASK ----------    create a custom role using a YAML file

title: "orca_storage_update"
description: "Permissions"
stage: "ALPHA"
includedPermissions:
- storage.buckets.get
- storage.objects.get
- storage.objects.list
- storage.objects.update
- storage.objects.create

gcloud iam roles create orca_storage_update --project $DEVSHELL_PROJECT_ID \
--file role-definition.yaml


4TH TASK ----------  Create and configure a new Kubernetes Engine private cluster


gcloud container clusters create orca-test-cluster --num-nodes 1 --master-ipv4-cidr=172.16.0.64/28 --network orca-build-vpc --subnetwork orca-build-subnet --enable-master-authorized-networks  --master-authorized-networks 192.168.10.2/32 --enable-ip-alias --enable-private-nodes --enable-private-endpoint --service-account orca-private-cluster-sa@<Project ID>.iam.gserviceaccount.com --zone us-east1-b

5th Task ---------- Deploy an application to a private Kubernetes Engine cluster

gcloud container clusters get-credentials orca-test-cluster --internal-ip --zone us-east1-b --project <Project ID>

+++++++++++++++++++++++++++++++++++++++++++


Step 1 :

nano role-definition.yaml

title: "orca_storage_controller_752" 
description: "Permissions"
stage: "ALPHA"
includedPermissions:
- storage.buckets.get
- storage.objects.get
- storage.objects.list
- storage.objects.update
- storage.objects.create


Ctrl + X --> Y --> Enter

gcloud iam roles create orca_storage_controller_752 --project $DEVSHELL_PROJECT_ID \
--file role-definition.yaml

Step 2 :

gcloud iam service-accounts create orca-private-cluster-840-sa \
   --display-name "orca-private-cluster-840-sa"




Step 3:
gcloud projects add-iam-policy-binding $DEVSHELL_PROJECT_ID \
   --member serviceAccount:orca-private-cluster-840-sa-sa@$DEVSHELL_PROJECT_ID.iam.gserviceaccount.com --role projects/$DEVSHELL_PROJECT_ID/roles/orca_storage_controller_752

gcloud projects add-iam-policy-binding $DEVSHELL_PROJECT_ID \
   --member serviceAccount:orca-private-cluster-840-sa-sa@$DEVSHELL_PROJECT_ID.iam.gserviceaccount.com --role roles/monitoring.viewer

gcloud projects add-iam-policy-binding $DEVSHELL_PROJECT_ID \
   --member serviceAccount:orca-private-cluster-840-sa-sa@$DEVSHELL_PROJECT_ID.iam.gserviceaccount.com --role roles/monitoring.metricWriter

gcloud projects add-iam-policy-binding $DEVSHELL_PROJECT_ID \
   --member serviceAccount:orca-private-cluster-840-sa-sa@$DEVSHELL_PROJECT_ID.iam.gserviceaccount.com --role roles/logging.logWriter



 


Step 4:

gcloud container clusters create orca-cluster-722 \
--num-nodes 1 --master-ipv4-cidr=172.16.0.64/28 --network orca-build-vpc --subnetwork orca-build-subnet --enable-master-authorized-networks  \
--master-authorized-networks 192.168.10.2/32 --enable-ip-alias --enable-private-nodes --enable-private-endpoint \
--service-account orca-private-cluster-840-sa-sa@qwiklabs-gcp-00-00c79d1c403c.iam.gserviceaccount.com --zone us-east1-b


Step 5:

Navigate to Compute Engine in the Cloud Console.
Click on the SSH button for the orca-jumphost instance.
In the SSH window, connect to the private cluster by running the following:

gcloud container clusters get-credentials orca-cluster-722 --internal-ip --zone us-east1-b --project qwiklabs-gcp-00-00c79d1c403c

kubectl create deployment hello-server --image=gcr.io/google-samples/hello-app:1.0

kubectl expose deployment hello-server --name orca-cluster-722 \
   --type LoadBalancer --port 80 --target-port 8080
   
   
   
   
   
